import argparse
import time
import asyncio
import os

from playwright.sync_api import sync_playwright, TimeoutError

from app.config import (
    TARGET_URL,
    HEADLESS,
    PAGE_WAIT,
    PAGE_LOAD_TIMEOUT,
    NEXT_BUTTON_SELECTOR,
)

from app.downloader import download_all
from app.exporter import export_to_json


# =========================
# PLAYWRIGHT FIX (Báº®T BUá»˜C)
# =========================
# Ã‰p Playwright KHÃ”NG dÃ¹ng browser ná»™i bá»™
os.environ["PLAYWRIGHT_BROWSERS_PATH"] = "0"


def get_chrome_path():
    """Tá»± Ä‘á»™ng tÃ¬m Chrome trÃªn Windows"""
    paths = [
        r"C:\Program Files\Google\Chrome\Application\chrome.exe",
        r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
    ]
    for path in paths:
        if os.path.exists(path):
            return path
    raise FileNotFoundError("âŒ KhÃ´ng tÃ¬m tháº¥y Google Chrome trÃªn mÃ¡y")


# =========================
# CRAWL LOGIC
# =========================
def extract_images(page, start_index=1):
    results = []
    stt = start_index

    cards = page.query_selector_all("div.card")
    for card in cards:
        img = card.query_selector("img")
        title_el = card.query_selector("h3.title")

        if not img or not title_el:
            continue

        src = img.get_attribute("src") or img.get_attribute("data-src")
        if not src or not src.startswith("http"):
            continue

        title = title_el.inner_text().strip()

        results.append({
            "stt": stt,
            "title": title,
            "url": src
        })
        stt += 1

    return results


def click_next(page) -> bool:
    try:
        icon = page.locator(NEXT_BUTTON_SELECTOR).first
        btn = icon.locator("..")

        if btn.get_attribute("disabled") is not None:
            return False

        btn.click()
        return True

    except Exception:
        return False


def crawl_pages(start_page, end_page, stop_flag=None):
    all_items = []
    global_stt = 1

    chrome_path = get_chrome_path()

    with sync_playwright() as p:
        browser = p.chromium.launch(
            executable_path=chrome_path,
            headless=HEADLESS,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-infobars",
                "--no-sandbox",
            ],
        )

        page = browser.new_page()

        print(f"Má»Ÿ trang: {TARGET_URL}")
        page.goto(TARGET_URL, timeout=PAGE_LOAD_TIMEOUT)
        page.wait_for_load_state("networkidle")

        # ðŸ”¥ NHáº¢Y Tá»šI START_PAGE
        current_page = 1
        while current_page < start_page:
            if stop_flag and stop_flag():
                print("[STOP] ÄÃ£ dá»«ng khi nháº£y trang")
                browser.close()
                return []

            print(f"Äang bá» qua TRANG {current_page}")
            if not click_next(page):
                print("KhÃ´ng thá»ƒ nháº£y tá»›i trang báº¯t Ä‘áº§u")
                browser.close()
                return []

            time.sleep(PAGE_WAIT)
            current_page += 1

        # ðŸ”¥ Báº®T Äáº¦U CRAWL
        while current_page <= end_page:
            if stop_flag and stop_flag():
                print("[STOP] ÄÃ£ dá»«ng crawl")
                break

            print(f"\n=== Äang crawl TRANG {current_page} ===")

            items = extract_images(page, start_index=global_stt)
            print(f"TÃ¬m tháº¥y {len(items)} áº£nh")

            all_items.extend(items)
            global_stt += len(items)

            if current_page == end_page:
                break

            if not click_next(page):
                break

            time.sleep(PAGE_WAIT)
            current_page += 1

        browser.close()

    return all_items


def run_crawler(start_page, end_page, stop_flag=None):
    items = crawl_pages(start_page, end_page, stop_flag)

    if stop_flag and stop_flag():
        print("[STOP] Dá»«ng trÆ°á»›c khi táº£i áº£nh")
        return

    downloaded_data = asyncio.run(download_all(items))

    if downloaded_data:
        export_to_json(downloaded_data)
    else:
        print("KhÃ´ng cÃ³ áº£nh má»›i (táº¥t cáº£ Ä‘Ã£ tá»“n táº¡i)")


# =========================
# CLI
# =========================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Crawler buavl.net (STT + title + image)"
    )

    parser.add_argument("--start", type=int, required=True)
    parser.add_argument("--end", type=int, required=True)

    args = parser.parse_args()

    if args.start < 1 or args.start > args.end:
        print("Trang khÃ´ng há»£p lá»‡")
        exit(1)

    run_crawler(args.start, args.end)