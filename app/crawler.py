import argparse
import time
import asyncio
import os

from playwright.sync_api import sync_playwright, TimeoutError

from app.config import (
    TARGET_URL,
    HEADLESS,
    PAGE_WAIT,
    PAGE_LOAD_TIMEOUT,
    NEXT_BUTTON_SELECTOR,
)

from app.downloader import download_all
from app.exporter import export_to_json


# =========================
# PLAYWRIGHT FIX (B·∫ÆT BU·ªòC)
# =========================
# √âp Playwright KH√îNG d√πng browser n·ªôi b·ªô
os.environ["PLAYWRIGHT_BROWSERS_PATH"] = "0"


def get_chrome_path():
    """T·ª± ƒë·ªông t√¨m Chrome tr√™n Windows"""
    paths = [
        r"C:\Program Files\Google\Chrome\Application\chrome.exe",
        r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
    ]
    for path in paths:
        if os.path.exists(path):
            return path
    raise FileNotFoundError("‚ùå Kh√¥ng t√¨m th·∫•y Google Chrome tr√™n m√°y")


# =========================
# CRAWL LOGIC
# =========================
def extract_images(page, start_index=1):
    """
    Tr√≠ch xu·∫•t ·∫£nh th√¥ng minh (Deep Scanning)
    Qu√©t t·∫•t c·∫£ c√°c th·∫ª <img> v√† c√°c thu·ªôc t√≠nh ti·ªÅm nƒÉng.
    """
    # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ·∫£nh lazy-load k·ªãp hi·ªán di·ªán trong DOM
    time.sleep(1)
    
    results = []
    stt = start_index
    imgs = page.query_selector_all("img")
    base_url = page.url

    for img in imgs:
        # L·∫•y T·∫§T C·∫¢ thu·ªôc t√≠nh c·ªßa th·∫ª img ƒë·ªÉ t√¨m link ·∫£nh
        all_attrs = page.evaluate('''(el) => {
            let attrs = {};
            for (let i = 0; i < el.attributes.length; i++) {
                attrs[el.attributes[i].name] = el.attributes[i].value;
            }
            return attrs;
        }''', img)

        src = None
        # ∆Øu ti√™n c√°c thu·ªôc t√≠nh ph·ªï bi·∫øn
        for attr in ["data-src", "data-original", "data-lazy-src", "srcset", "src", "data-lazy"]:
            if attr in all_attrs and all_attrs[attr]:
                val = all_attrs[attr]
                if val.strip().startswith("http") or val.strip().startswith("/") or val.strip().startswith("//"):
                    src = val
                    break
        
        # N·∫øu v·∫´n kh√¥ng th·∫•y, qu√©t s·∫°ch to√†n b·ªô thu·ªôc t√≠nh t√¨m link ·∫£nh
        if not src:
            for attr_name, attr_val in all_attrs.items():
                if isinstance(attr_val, str) and (attr_val.endswith((".jpg", ".png", ".jpeg", ".gif", ".webp")) or "http" in attr_val):
                    src = attr_val
                    break

        if not src:
            continue
            
        # X·ª≠ l√Ω srcset
        if " " in src and "," in src:
            src = src.split(",")[0].split(" ")[0]
        elif " " in src:
            src = src.split(" ")[0]

        # B·ªô l·ªçc r√°c th√¥ng minh
        img_id = (all_attrs.get("id") or "").lower()
        img_class = (all_attrs.get("class") or "").lower()
        alt_text = (all_attrs.get("alt") or "").lower()
        
        # Ch·ªâ ch·∫∑n r√°c h·ªá th·ªëng th·ª±c s·ª± (icon nh·ªè, avatar m·∫∑c ƒë·ªãnh)
        is_junk = any(kw in (img_id + img_class + alt_text + src.lower()) 
                     for kw in ["favicon", "icon-", "tracker", "ads-", "advertisement"])

        if is_junk:
            continue

        # L·ªçc k√≠ch th∆∞·ªõc (N·ªõi l·ªèng ƒë·ªÉ kh√¥ng m·∫•t ·∫£nh meme)
        try:
            width = int(all_attrs.get("width") or 0)
            height = int(all_attrs.get("height") or 0)
            if (width > 0 and width < 60) or (height > 0 and height < 60):
                continue
        except: pass

        # Chu·∫©n h√≥a URL
        if src.startswith("//"):
            src = "https:" + src
        elif src.startswith("/"):
            from urllib.parse import urljoin
            src = urljoin(base_url, src)
        elif not src.startswith("http"):
            continue

        title = all_attrs.get("alt") or all_attrs.get("title") or f"Image_{stt}"
        title = title.strip()[:100]

        results.append({
            "stt": stt,
            "title": title,
            "url": src
        })
        stt += 1

    # Lo·∫°i b·ªè link tr√πng l·∫∑p
    seen_urls = set()
    unique_results = []
    for item in results:
        if item["url"] not in seen_urls:
            unique_results.append(item)
            seen_urls.add(item["url"])
            
    return unique_results


def click_next_or_scroll(page) -> bool:
    """
    Th√¥ng minh: T√¨m n√∫t Ti·∫øp, Xem th√™m ho·∫∑c Cu·ªôn xu·ªëng n·∫øu kh√¥ng c√≥ n√∫t.
    """
    try:
        # 1. T√¨m c√°c n√∫t c√≥ ch·ªØ "Ti·∫øp", "Next", "Xem th√™m", "More"
        selectors = [
            "text='Trang ti·∫øp'", "text='Trang sau'", "text='Next'", 
            "text='Xem th√™m'", "text='Load more'", "text='More'",
            "span.next-icon", "button.load-more", "i.fas.fa-chevron-right", "i.fa-chevron-right"
        ]
        
        for sel in selectors:
            btn = page.query_selector(sel)
            if btn and btn.is_visible() and btn.is_enabled():
                btn.click()
                print(f"[Smart] Chuy·ªÉn trang ti·∫øp theo")
                return True

        # 2. N·∫øu kh√¥ng c√≥ n√∫t, th·ª≠ cu·ªôn xu·ªëng (Progressive Scroll)
        print("[Smart] Kh√¥ng th·∫•y n√∫t, ƒëang th·ª≠ cu·ªôn trang b·∫≠c thang...")
        previous_height = page.evaluate("document.body.scrollHeight")
        
        # Cu·ªôn 3 l·∫ßn, m·ªói l·∫ßn m·ªôt ƒëo·∫°n ƒë·ªÉ k√≠ch ho·∫°t lazy-load
        for i in range(1, 4):
            scroll_to = (previous_height // 3) * i
            page.evaluate(f"window.scrollTo(0, {scroll_to})")
            time.sleep(1)
        
        # ƒê·ª£i th√™m m·ªôt ch√∫t ƒë·ªÉ n·ªôi dung m·ªõi n·∫°p h·∫≥n
        time.sleep(2)
        
        new_height = page.evaluate("document.body.scrollHeight")
        if new_height > previous_height:
            print("[Smart] ƒê√£ n·∫°p th√†nh c√¥ng n·ªôi dung m·ªõi.")
            return True
            
        return False

    except Exception as e:
        print(f"[Smart] L·ªói khi chuy·ªÉn n·ªôi dung: {e}")
        return False


def crawl_pages(start_page, end_page, target_url=TARGET_URL, stop_flag=None, progress_callback=None):
    all_items = []
    global_stt = 1

    chrome_path = get_chrome_path()

    with sync_playwright() as p:
        browser = p.chromium.launch(
            executable_path=chrome_path,
            headless=HEADLESS,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-infobars",
                "--no-sandbox",
            ],
        )

        page = browser.new_page()

        print(f"M·ªü trang: {target_url}")
        page.goto(target_url, timeout=PAGE_LOAD_TIMEOUT)
        page.wait_for_load_state("networkidle")

        # üî• NH·∫¢Y T·ªöI START_PAGE
        current_page = 1
        while current_page < start_page:
            if stop_flag and stop_flag():
                print("[STOP] ƒê√£ d·ª´ng khi nh·∫£y trang")
                browser.close()
                return []

            print(f"ƒêang b·ªè qua TRANG {current_page}")
            if not click_next_or_scroll(page):
                print("Kh√¥ng th·ªÉ nh·∫£y t·ªõi trang b·∫Øt ƒë·∫ßu")
                browser.close()
                return []

            time.sleep(PAGE_WAIT)
            current_page += 1

        # üî• B·∫ÆT ƒê·∫¶U CRAWL
        while current_page <= end_page:
            if stop_flag and stop_flag():
                print("[STOP] ƒê√£ d·ª´ng crawl")
                break

            print(f"\n=== ƒêang crawl TRANG {current_page} ===")

            # Report progress
            if progress_callback:
                progress_callback(current_page, f"ƒêang crawl trang {current_page}/{end_page}")

            items = extract_images(page, start_index=global_stt)
            print(f"T√¨m th·∫•y {len(items)} ·∫£nh")

            all_items.extend(items)
            global_stt += len(items)

            if current_page == end_page:
                break

            if not click_next_or_scroll(page):
                print("[!] Kh√¥ng th·∫•y trang ti·∫øp theo ho·∫∑c kh√¥ng th·ªÉ cu·ªôn th√™m.")
                break

            time.sleep(PAGE_WAIT)
            current_page += 1

        browser.close()

    return all_items


def run_crawler(start_page, end_page, target_url=TARGET_URL, stop_flag=None, progress_callback=None):
    items = crawl_pages(start_page, end_page, target_url=target_url, stop_flag=stop_flag, progress_callback=progress_callback)

    if stop_flag and stop_flag():
        print("[STOP] D·ª´ng tr∆∞·ªõc khi t·∫£i ·∫£nh")
        return

    # Report downloading phase
    if progress_callback:
        progress_callback(end_page, "T·∫£i ·∫£nh...")

    downloaded_data = asyncio.run(download_all(items))

    if downloaded_data:
        export_to_json(downloaded_data)
    else:
        print("Kh√¥ng c√≥ ·∫£nh m·ªõi (t·∫•t c·∫£ ƒë√£ t·ªìn t·∫°i)")


# =========================
# CLI
# =========================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Crawler buavl.net (STT + title + image)"
    )

    parser.add_argument("--start", type=int, required=True)
    parser.add_argument("--end", type=int, required=True)

    args = parser.parse_args()

    if args.start < 1 or args.start > args.end:
        print("Trang kh√¥ng h·ª£p l·ªá")
        exit(1)

    run_crawler(args.start, args.end)